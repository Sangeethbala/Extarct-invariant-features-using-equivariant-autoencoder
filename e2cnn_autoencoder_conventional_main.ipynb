{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import sys\n",
    "# sys.path.append(\"/home/sbalakri/PycharmProjects/PhD_1st_sem/e2cnn\")\n",
    "# from e2cnn import gspaces\n",
    "# from e2cnn import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "hidden_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(Conv_CNN, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 3\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 4\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(4, 1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=4)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # convolution decoder 1 block 1\n",
    "        self.block3_dec = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=2, stride=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.block4_dec = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2_dec = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=5/2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "       # convolution decoder 2 block 1\n",
    "        self.block1_dec = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )               \n",
    "        # convolution decoder 2 block 2\n",
    "        self.block2_dec = nn.Sequential(\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.pool1_dec = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=29/15)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def encode(self, input: torch.Tensor):\n",
    "        \n",
    "        x = input\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)         \n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        return x \n",
    "    \n",
    "    def decode(self, input: torch.Tensor):\n",
    "        \n",
    "        x = input\n",
    "        x = self.block3_dec(x)\n",
    "        x = self.block4_dec(x)\n",
    "        x = self.pool2_dec(x)\n",
    "        \n",
    "        x = self.block1_dec(x)\n",
    "        x = self.block2_dec(x)\n",
    "        x = self.pool1_dec(x)\n",
    "    \n",
    "        return x \n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "\n",
    "        x = input\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block3_dec(x)\n",
    "        x = self.block4_dec(x)\n",
    "        x = self.pool2_dec(x)\n",
    "        \n",
    "        x = self.block1_dec(x)\n",
    "        x = self.block2_dec(x)\n",
    "        x = self.pool1_dec(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘mnist_rotation_new.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  mnist_rotation_new.zip\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# uncompress the zip file\n",
    "!unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistRotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
    "        else:\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "        if mode == \"train\":    \n",
    "            self.images = data[:, :-1][:500].reshape(-1, 28, 28).astype(np.float32)\n",
    "            self.labels = data[:, -1][:500].astype(np.int64)\n",
    "        else:\n",
    "            self.images = data[:, :-1][:30000].reshape(-1, 28, 28).astype(np.float32)\n",
    "            self.labels = data[:, -1][:30000].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Conv_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (1, 29, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    \n",
    "    x = pad(x)\n",
    "    \n",
    "    print(x)\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            x_transformed = totensor(x.rotate(r*45., Image.BILINEAR)).reshape(1, 1, 29, 29)\n",
    "            x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()\n",
    "\n",
    "    \n",
    "# build the test set    \n",
    "mnist_test = MnistRotDataset(mode='test')\n",
    "\n",
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(mnist_test))\n",
    "\n",
    "# evaluate the model\n",
    "# test_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "import os\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "totensor = ToTensor()\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=16,\n",
    "                 shuffle=True)\n",
    "\n",
    "test_transform = Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "mnist_test = MnistRotDataset(mode='test', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=16)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.08661569517791567474\n",
      "0 0.08661569517791567\n",
      "1 0.0535745929315019\n",
      "2 0.048022759142354945\n",
      "3 0.04413232930115799\n",
      "4 0.04142974391208952\n",
      "[6,     0] loss: 0.03959260869826843748\n",
      "5 0.03959260869826844\n",
      "6 0.03741387418698316\n",
      "7 0.03680647712256482\n",
      "8 0.036074053375652775\n",
      "9 0.034603436537717115\n",
      "[11,     0] loss: 0.03410158927241960930\n",
      "10 0.03410158927241961\n",
      "11 0.033982008315659874\n",
      "12 0.033736320555358384\n",
      "13 0.03317086393968086\n",
      "14 0.03284355938731141\n",
      "[16,     0] loss: 0.03272702271554304293\n",
      "15 0.03272702271554304\n",
      "16 0.03219154077716431\n",
      "17 0.03215129692941459\n",
      "18 0.03178376872185154\n",
      "19 0.03185673639993763\n",
      "[21,     0] loss: 0.03194062441439177863\n",
      "20 0.03194062441439178\n",
      "21 0.03183569898133847\n",
      "22 0.03166331714065514\n",
      "23 0.03146551709751881\n",
      "24 0.031124192081503015\n",
      "[26,     0] loss: 0.03086214082602837736\n",
      "25 0.030862140826028377\n",
      "26 0.030659768962074275\n",
      "27 0.030762676742687747\n",
      "28 0.03075851059856996\n",
      "29 0.030566231389321498\n",
      "[31,     0] loss: 0.03016493642767566902\n",
      "30 0.03016493642767567\n",
      "31 0.030126628739323782\n",
      "32 0.030257143515778417\n",
      "33 0.030092944673711982\n",
      "34 0.02981113259731537\n",
      "[36,     0] loss: 0.03000225452011200805\n",
      "35 0.030002254520112008\n",
      "36 0.029968961952856525\n",
      "37 0.029666041269601873\n",
      "38 0.02916997919480006\n",
      "39 0.029212909607003578\n",
      "[41,     0] loss: 0.02916260971803570287\n",
      "40 0.029162609718035703\n",
      "41 0.029241939376465124\n",
      "42 0.029448749200992323\n",
      "43 0.0293219210440988\n",
      "44 0.029183618965285336\n",
      "[46,     0] loss: 0.02965822339576868263\n",
      "45 0.029658223395768683\n",
      "46 0.029552594327659748\n",
      "47 0.029266565482118236\n",
      "48 0.029302946083358865\n",
      "49 0.02906626323933032\n",
      "1759.8305366039276 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "epoch = 0\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    num_samples = 0\n",
    "    for i, (x, t) in enumerate(train_loader):\n",
    "        \n",
    "        if i>200:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        loss = loss_function(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_samples = num_samples + 1\n",
    "    if epoch % 5 == 0:   # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.20f' %\n",
    "              (epoch + 1, 0 , running_loss / num_samples))\n",
    "              \n",
    "    print(epoch, running_loss / num_samples)\n",
    "    \n",
    "print('{} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = torch.tensor([])\n",
    "t_all = torch.tensor([])\n",
    "for i, (x, t) in enumerate(train_loader):\n",
    "\n",
    "    y = model.encode(x)\n",
    "    enc_rep=torch.cat((enc_rep, y), 0)\n",
    "\n",
    "    t = t.float()\n",
    "    t_all = torch.cat((t_all, t), 0)\n",
    "    \n",
    "    if i > 200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export  \n",
    "enc_rep_all = enc_rep.reshape(-1, hidden_channels*16).detach().numpy()\n",
    "import pickle\n",
    "features_path = \"conv_autoe_features_conv_train.pickle\"\n",
    "labels_path = \"conv_autoe_labels_conv_train.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_conv_train.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = torch.tensor([])\n",
    "t_all = torch.tensor([])\n",
    "for i, (x, t) in enumerate(test_loader):\n",
    "\n",
    "    y = model.encode(x)\n",
    "    enc_rep=torch.cat((enc_rep, y), 0)\n",
    "\n",
    "    t = t.float()\n",
    "    t_all = torch.cat((t_all, t), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export  \n",
    "enc_rep_all = enc_rep.reshape(-1, hidden_channels*16).detach().numpy()\n",
    "import pickle\n",
    "features_path = \"conv_autoe_features_conv_test.pickle\"\n",
    "labels_path = \"conv_autoe_labels_conv_test.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_conv_test.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 8, 49]' is invalid for input of size 480000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-34a1347ee8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#gram pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menc_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoded_rep_all_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mencoded_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 8, 49]' is invalid for input of size 480000"
     ]
    }
   ],
   "source": [
    "#gram pooling\n",
    "enc_rep = enc_rep.reshape(-1, hidden_channels*8, 49)\n",
    "encoded_rep_all_sum = torch.tensor([])\n",
    "for s in range(enc_rep.shape[0]):   \n",
    "    encoded_rep = enc_rep[s]\n",
    "    all_encs =  torch.tensor([])\n",
    "    sum_enc_rep = torch.empty(hidden_channels, hidden_channels, dtype=torch.float)\n",
    "    for mm in range(hidden_channels):\n",
    "        m = 8 * mm\n",
    "        for i in range(hidden_channels - mm):\n",
    "            sum_enc_rep[mm, i+mm] = torch.dot(encoded_rep[m], encoded_rep[8*i + m]) + torch.dot(encoded_rep[m+1],encoded_rep[8*i + m+1]) + torch.dot(encoded_rep[m+2], encoded_rep[8*i + m+2]) + torch.dot(encoded_rep[m+3], encoded_rep[8*i + m+3]) + torch.dot(encoded_rep[m+4], encoded_rep[8*i + m+4]) + torch.dot(encoded_rep[m+5],encoded_rep[8*i + m+5]) + torch.dot(encoded_rep[m+6],encoded_rep[8*i + m+6]) + torch.dot(encoded_rep[m+7],encoded_rep[8*i + m+7])\n",
    "            sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\n",
    "    encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, hidden_channels*hidden_channels)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_rep_all = encoded_rep_all_sum.reshape(-1, hidden_channels*hidden_channels).detach().numpy()\n",
    "# features_path = \"conv_autoe_features_test2.pickle\"\n",
    "# labels_path = \"conv_autoe_labels_test2.pickle\"\n",
    "# orig_path = \"conv_autoe_orig_features_test2.pickle\"\n",
    "# pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "# t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "# pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_rep_all = encoded_rep_all_sum.reshape(-1, hidden_channels*hidden_channels).detach().numpy()\n",
    "# features_path = \"conv_autoe_features_train2.pickle\"\n",
    "# labels_path = \"conv_autoe_labels_train2.pickle\"\n",
    "# orig_path = \"conv_autoe_orig_features_train2.pickle\"\n",
    "# pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "# t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "# pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
