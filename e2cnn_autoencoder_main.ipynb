{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/sbalakri/PycharmProjects/PhD_1st_sem/e2cnn\")\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn\n",
    "from matplotlib import pyplot as plt\n",
    "hidden_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=8)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution encoder 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 16 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 16*[self.r2_act.regular_repr])\n",
    "        self.block1 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, maximum_offset=0),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 32 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 8*[self.r2_act.regular_repr])\n",
    "        self.block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, maximum_offset=0),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.SequentialModule(\n",
    "            nn.PointwiseMaxPool(out_type, kernel_size=3, padding=1, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 32 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 4*[self.r2_act.regular_repr])\n",
    "        self.block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, maximum_offset=0),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution encoder 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 1*[self.r2_act.regular_repr])\n",
    "        self.block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, maximum_offset=0),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.SequentialModule(\n",
    "            nn.PointwiseMaxPool(out_type, kernel_size=3, padding=1, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.input_type1 = self.block4.out_type\n",
    "                \n",
    "        # convolution decoder 1 block 1\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # The output type of the third convolution layer are 32 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 4*[self.r2_act.regular_repr])\n",
    "        self.block3_dec = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=2, stride=1),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # convolution ddecoder 1 block 2 \n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3_dec.out_type\n",
    "        # the output type of the fourth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, 8*[self.r2_act.regular_repr])\n",
    "        self.block4_dec = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, stride=1),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2_dec = nn.SequentialModule(\n",
    "            nn.R2Upsampling(out_type, scale_factor=5/2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "       # convolution decoder 2 block 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 16 feature fields, each transforming under the regular representation of C8\n",
    "        in_type = self.block4_dec.out_type\n",
    "        out_type = nn.FieldType(self.r2_act, 16*[self.r2_act.regular_repr])\n",
    "        self.block1_dec = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, stride=1),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution decoder 2 block 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1_dec.out_type\n",
    "        # the output type of the second convolution layer are 32 regular feature fields of C8\n",
    "        out_type = nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.block2_dec = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=3, padding=1, stride=1),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "     \n",
    "        self.pool1_dec = nn.SequentialModule(\n",
    "            nn.R2Upsampling(out_type, scale_factor=29/15)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def encode(self, input: torch.Tensor):\n",
    "        \n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "    def decode(self, input: torch.Tensor):\n",
    "        \n",
    "        x = nn.GeometricTensor(input, self.input_type1)\n",
    "        x = self.block3_dec(x)\n",
    "        x = self.block4_dec(x)\n",
    "        x = self.pool2_dec(x)\n",
    "        \n",
    "        x = self.block1_dec(x)\n",
    "        x = self.block2_dec(x)\n",
    "        x = self.pool1_dec(x)\n",
    "    \n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)        \n",
    "\n",
    "        x = self.block3_dec(x)\n",
    "        x = self.block4_dec(x)\n",
    "        x = self.pool2_dec(x)\n",
    "        \n",
    "        x = self.block1_dec(x)\n",
    "        x = self.block2_dec(x)\n",
    "        x = self.pool1_dec(x)\n",
    "\n",
    "        \n",
    "        # pool over the group\n",
    "#         x = self.gpool(x)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "#         x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘mnist_rotation_new.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  mnist_rotation_new.zip\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# uncompress the zip file\n",
    "!unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistRotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
    "        else:\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "        if mode == \"train\":    \n",
    "            self.images = data[:, :-1][:500].reshape(-1, 28, 28).astype(np.float32)\n",
    "            self.labels = data[:, -1][:500].astype(np.int64)\n",
    "        else:\n",
    "            self.images = data[:, :-1][:30000].reshape(-1, 28, 28).astype(np.float32)\n",
    "            self.labels = data[:, -1][:30000].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "model = C8SteerableCNN().to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()# from torchsummary import summary\n",
    "# summary(model, (1, 29, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: torch.nn.Module, x: Image):\n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    model.eval()\n",
    "    \n",
    "    x = pad(x)\n",
    "    \n",
    "    print(x)\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    with torch.no_grad():\n",
    "        for r in range(8):\n",
    "            x_transformed = totensor(x.rotate(r*45., Image.BILINEAR)).reshape(1, 1, 29, 29)\n",
    "            x_transformed.to(device)\n",
    "\n",
    "            y = model(x_transformed)\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "            \n",
    "            angle = r * 45\n",
    "            print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()\n",
    "\n",
    "    \n",
    "# build the test set    \n",
    "mnist_test = MnistRotDataset(mode='test')\n",
    "\n",
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(mnist_test))\n",
    "\n",
    "# evaluate the model\n",
    "# test_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "import os\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "totensor = ToTensor()\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "\n",
    "# mnist_train = MnistRotDataset(mode='train', transform=train_transform)\n",
    "# train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=32)\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=16,\n",
    "                 shuffle=True)\n",
    "\n",
    "test_transform = Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "mnist_test = MnistRotDataset(mode='test', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=16)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.29277355562556872659\n",
      "0 0.009149173613299023\n",
      "6.215287923812866 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "epoch = 0\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    num_samples = 0\n",
    "    for i, (x, t) in enumerate(train_loader):\n",
    "        \n",
    "        if i>10:\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        loss = loss_function(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_samples = num_samples + 1\n",
    "    if epoch % 5 == 0:   # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.20f' %\n",
    "              (epoch + 1, 0 , running_loss / num_samples))\n",
    "              \n",
    "    print(epoch, running_loss / (num_samples*32))\n",
    "    \n",
    "print('{} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t0 = time.time()\n",
    "# num_samples = 0\n",
    "# epoch = 0\n",
    "# running_loss = 0\n",
    "# for i, (x, t) in enumerate(test_loader):\n",
    "#     y = model(x)\n",
    "#     loss = loss_function(y, x)\n",
    "\n",
    "#     running_loss += loss.item()\n",
    "#     num_samples = num_samples + 1\n",
    "    \n",
    "# print('[%d, %5d] loss: %.20f' %\n",
    "#       (1, 0 , running_loss / num_samples))\n",
    "# running_loss = 0.0\n",
    "\n",
    "\n",
    "# print('{} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_mnist_test = MnistRotDataset(mode='test')\n",
    "# x_image, y_label = other_mnist_test.__getitem__(5)\n",
    "\n",
    "# x = pad(x_image)\n",
    "# x_transformed = totensor(x).reshape(1, 1, 29, 29)\n",
    "# y = model(x_transformed)\n",
    "\n",
    "# dd = x_transformed.detach().numpy()[0][0]\n",
    "# prediction = y.detach().numpy()\n",
    "# kk = prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = torch.tensor([])\n",
    "t_all = torch.tensor([])\n",
    "for i, (x, t) in enumerate(train_loader):\n",
    "    \n",
    "    y = model.encode(x)\n",
    "    y = y.tensor\n",
    "    enc_rep=torch.cat((enc_rep, y), 0)\n",
    "\n",
    "    t = t.float()\n",
    "    t_all = torch.cat((t_all, t), 0)\n",
    "    if i > 200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, (x, t) in enumerate(test_loader):\n",
    "\n",
    "#     y = model.encode(x)\n",
    "#     y = y.tensor\n",
    "#     decoded = model.decode(y)\n",
    "#     if i > 2:\n",
    "#         decoded = decoded.tensor\n",
    "#         decoded1 = decoded.detach().numpy()\n",
    "\n",
    "#         plt.imshow(decoded1[20].reshape(29,29))\n",
    "#         plt.show()\n",
    "#         plt.imshow(x[20].reshape(29,29))\n",
    "#         break\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = enc_rep.reshape(-1, 1 * 8, 16)\n",
    "encoded_rep_all = torch.tensor([])\n",
    "for s in range(enc_rep.shape[0]):   \n",
    "    encoded_rep = enc_rep[s]\n",
    "    all_encs =  torch.tensor([])\n",
    "    for m in range(hidden_channels):\n",
    "        sum_enc_rep = 0\n",
    "        for i in range(8):\n",
    "            sum_enc_rep = sum_enc_rep + encoded_rep[8 * m + i]\n",
    "        all_encs = torch.cat((all_encs, sum_enc_rep.reshape(1, 16)), 0)\n",
    "        all_encs = all_encs/8.\n",
    "    encoded_rep_all = torch.cat((encoded_rep_all, all_encs.reshape(1, all_encs.shape[0], all_encs.shape[1])), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export  \n",
    "enc_rep_all = encoded_rep_all.reshape(-1, hidden_channels*16).detach().numpy()\n",
    "import pickle\n",
    "features_path = \"conv_autoe_features_train.pickle\"\n",
    "labels_path = \"conv_autoe_labels_train.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_train.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, got 0D, 0D tensors at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:774",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e552ffecaae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#     for i in range(hidden_channels - mm):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoded_rep_all_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep_all_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, got 0D, 0D tensors at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:774"
     ]
    }
   ],
   "source": [
    "enc_rep = enc_rep.reshape(-1, 1*8, 16)\n",
    "encoded_rep_all_sum = torch.tensor([])\n",
    "for s in range(enc_rep.shape[0]):   \n",
    "    encoded_rep = enc_rep[s]\n",
    "    all_encs =  torch.tensor([])\n",
    "    sum_enc_rep = torch.empty(16, 16, dtype=torch.float)\n",
    "    m = 0\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "        #     for i in range(hidden_channels - mm):\n",
    "            sum_enc_rep[i, j] = torch.dot(encoded_rep[m][i], encoded_rep[m][j]) + torch.dot(encoded_rep[m+1][i],encoded_rep[m+1][j]) + torch.dot(encoded_rep[m+2][i], encoded_rep[m+2][j]) + torch.dot(encoded_rep[m+3][i], encoded_rep[m+3][j]) + torch.dot(encoded_rep[m+4][i], encoded_rep[m+4][j]) + torch.dot(encoded_rep[m+5][i],encoded_rep[m+5][j]) + torch.dot(encoded_rep[m+6][i],encoded_rep[m+6][j]) + torch.dot(encoded_rep[m+7][i],encoded_rep[m+7][j])\n",
    "#             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]  \n",
    "    encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, 16, 16)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(encoded_rep[m][i], encoded_rep[m][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep_all = sum_enc_rep.reshape(-1, 16*16).detach().numpy()\n",
    "features_path = \"conv_autoe_features_train_gram.pickle\"\n",
    "labels_path = \"conv_autoe_labels_train_gram.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_train_gram.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = torch.tensor([])\n",
    "t_all = torch.tensor([])\n",
    "for i, (x, t) in enumerate(test_loader):\n",
    "    \n",
    "    y = model.encode(x)\n",
    "    y = y.tensor\n",
    "    enc_rep=torch.cat((enc_rep, y), 0)\n",
    "\n",
    "    t = t.float()\n",
    "    t_all = torch.cat((t_all, t), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = enc_rep.reshape(-1, 1 * 8, 16)\n",
    "encoded_rep_all = torch.tensor([])\n",
    "for s in range(enc_rep.shape[0]):   \n",
    "    encoded_rep = enc_rep[s]\n",
    "    all_encs =  torch.tensor([])\n",
    "    for m in range(hidden_channels):\n",
    "        sum_enc_rep = 0\n",
    "        for i in range(8):\n",
    "            sum_enc_rep = sum_enc_rep + encoded_rep[8 * m + i]\n",
    "        all_encs = torch.cat((all_encs, sum_enc_rep.reshape(1, 16)), 0)\n",
    "        all_encs = all_encs/8.\n",
    "    encoded_rep_all = torch.cat((encoded_rep_all, all_encs.reshape(1, all_encs.shape[0], all_encs.shape[1])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export  \n",
    "enc_rep_all = encoded_rep_all.reshape(-1, 16).detach().numpy()\n",
    "import pickle\n",
    "features_path = \"conv_autoe_features_test.pickle\"\n",
    "labels_path = \"conv_autoe_labels_test.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_test.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep = enc_rep.reshape(-1, 1*8, 16)\n",
    "encoded_rep_all_sum = torch.tensor([])\n",
    "for s in range(enc_rep.shape[0]):   \n",
    "    encoded_rep = enc_rep[s]\n",
    "    all_encs =  torch.tensor([])\n",
    "    sum_enc_rep = torch.empty(16, 16, dtype=torch.float)\n",
    "    m = 0\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "        #     for i in range(hidden_channels - mm):\n",
    "            sum_enc_rep[i, j] = torch.dot(encoded_rep[m][i], encoded_rep[m][j]) + torch.dot(encoded_rep[m+1][i],encoded_rep[m+1][j]) + torch.dot(encoded_rep[m+2][i], encoded_rep[m+2][j]) + torch.dot(encoded_rep[m+3][i], encoded_rep[m+3][j]) + torch.dot(encoded_rep[m+4][i], encoded_rep[m+4][j]) + torch.dot(encoded_rep[m+5][i],encoded_rep[m+5][j]) + torch.dot(encoded_rep[m+6][i],encoded_rep[m+6][j]) + torch.dot(encoded_rep[m+7][i],encoded_rep[m+7][j])\n",
    "#             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\n",
    "    encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, 16, 16)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_rep_all = sum_enc_rep.reshape(-1, 16*16).detach().numpy()\n",
    "features_path = \"conv_autoe_features_test_gram.pickle\"\n",
    "labels_path = \"conv_autoe_labels_test_gram.pickle\"\n",
    "orig_path = \"conv_autoe_orig_features_test_gram.pickle\"\n",
    "pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #gram pooling\n",
    "# enc_rep = enc_rep.reshape(-1, hidden_channels*8, 49)\n",
    "# encoded_rep_all_sum = torch.tensor([])\n",
    "# for s in range(enc_rep.shape[0]):   \n",
    "#     encoded_rep = enc_rep[s]\n",
    "#     all_encs =  torch.tensor([])\n",
    "#     sum_enc_rep = torch.empty(hidden_channels, hidden_channels, dtype=torch.float)\n",
    "#     for mm in range(hidden_channels):\n",
    "#         m = 8 * mm\n",
    "#         for i in range(hidden_channels - mm):\n",
    "#             sum_enc_rep[mm, i+mm] = torch.dot(encoded_rep[m], encoded_rep[8*i + m]) + torch.dot(encoded_rep[m+1],encoded_rep[8*i + m+1]) + torch.dot(encoded_rep[m+2], encoded_rep[8*i + m+2]) + torch.dot(encoded_rep[m+3], encoded_rep[8*i + m+3]) + torch.dot(encoded_rep[m+4], encoded_rep[8*i + m+4]) + torch.dot(encoded_rep[m+5],encoded_rep[8*i + m+5]) + torch.dot(encoded_rep[m+6],encoded_rep[8*i + m+6]) + torch.dot(encoded_rep[m+7],encoded_rep[8*i + m+7])\n",
    "#             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\n",
    "#     encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, hidden_channels*hidden_channels)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, got 0D, 0D tensors at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:774",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-43f0eab80e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_channels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoded_rep_all_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_rep_all_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_enc_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, got 0D, 0D tensors at /opt/conda/conda-bld/pytorch_1573049308701/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:774"
     ]
    }
   ],
   "source": [
    "#gram pooling\n",
    "# enc_rep = enc_rep.reshape(-1, 1*8* 16)\n",
    "# encoded_rep_all_sum = torch.tensor([])\n",
    "# for s in range(enc_rep.shape[0]):   \n",
    "#     encoded_rep = enc_rep[s]\n",
    "#     all_encs =  torch.tensor([])\n",
    "#     sum_enc_rep = torch.empty(hidden_channels, hidden_channels, dtype=torch.float)\n",
    "#     for mm in range(hidden_channels):\n",
    "#         m = 8 * mm\n",
    "#         for i in range(hidden_channels - mm):\n",
    "#             sum_enc_rep[mm, i+mm] = torch.dot(encoded_rep[m], encoded_rep[8*i + m]) + torch.dot(encoded_rep[m+1],encoded_rep[8*i + m+1]) + torch.dot(encoded_rep[m+2], encoded_rep[8*i + m+2]) + torch.dot(encoded_rep[m+3], encoded_rep[8*i + m+3]) + torch.dot(encoded_rep[m+4], encoded_rep[8*i + m+4]) + torch.dot(encoded_rep[m+5],encoded_rep[8*i + m+5]) + torch.dot(encoded_rep[m+6],encoded_rep[8*i + m+6]) + torch.dot(encoded_rep[m+7],encoded_rep[8*i + m+7])\n",
    "#             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\n",
    "#     encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, hidden_channels*hidden_channels)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_rep = enc_rep.reshape(-1, 1*8, 16)\n",
    "# encoded_rep_all_sum = torch.tensor([])\n",
    "# for s in range(enc_rep.shape[0]):   \n",
    "#     encoded_rep = enc_rep[s]\n",
    "#     all_encs =  torch.tensor([])\n",
    "#     sum_enc_rep = torch.empty(16, 16, dtype=torch.float)\n",
    "#     m = 0\n",
    "#     for i in range(16):\n",
    "#         for j in range(16):\n",
    "#         #     for i in range(hidden_channels - mm):\n",
    "#             sum_enc_rep[i, j] = torch.dot(encoded_rep[m][i], encoded_rep[m][j]) + torch.dot(encoded_rep[m+1][i],encoded_rep[m+1][j]) + torch.dot(encoded_rep[m+2][i], encoded_rep[m+2][j]) + torch.dot(encoded_rep[m+3][i], encoded_rep[m+3][j]) + torch.dot(encoded_rep[m+4][i], encoded_rep[m+4][j]) + torch.dot(encoded_rep[m+5][i],encoded_rep[m+5][j]) + torch.dot(encoded_rep[m+6][i],encoded_rep[m+6][j]) + torch.dot(encoded_rep[m+7][i],encoded_rep[m+7][j])\n",
    "# #             sum_enc_rep[i+mm, mm] = sum_enc_rep[mm, i+mm]\n",
    "# #     encoded_rep_all_sum = torch.cat((encoded_rep_all_sum, sum_enc_rep.reshape(1, 16, 16)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_rep_all = sum_enc_rep.reshape(-1, 16*16).detach().numpy()\n",
    "# features_path = \"conv_autoe_features_test_gram.pickle\"\n",
    "# labels_path = \"conv_autoe_labels_test_gram.pickle\"\n",
    "# orig_path = \"conv_autoe_orig_features_test_gram.pickle\"\n",
    "# pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "# t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "# pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_rep_all = sum_enc_rep.reshape(-1, hidden_channels*hidden_channels).detach().numpy()\n",
    "# features_path = \"conv_autoe_features_train_gram.pickle\"\n",
    "# labels_path = \"conv_autoe_labels_train_gram.pickle\"\n",
    "# orig_path = \"conv_autoe_orig_features_train_gram.pickle\"\n",
    "# pickle.dump(enc_rep_all, open(features_path, 'wb'))\n",
    "\n",
    "# t_all1 = t_all.detach().numpy().reshape(t_all.shape[0], 1)\n",
    "# pickle.dump(t_all1, open(labels_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_enc_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
